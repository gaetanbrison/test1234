# -*- coding: utf-8 -*-
"""Llma3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11pQHrDf41v5x0nn7iyHfVcLD9zgU1HLq
"""

# Colab Setup
!pip install transformers datasets peft huggingface_hub -q

# Utility Function: Hugging Face Login
def huggingface_login(hf_token):
    from huggingface_hub import login
    login(token=hf_token)

# Hugging Face Authentication
huggingface_token = "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"  # Replace with your Hugging Face token
huggingface_login(huggingface_token)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")  # Replace with your LLaMA model path

# Load the model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B")

# Tokenize input text
input_text = "What is the capital of France"
inputs = tokenizer(input_text, return_tensors="pt")

# Generate model outputs
outputs = model(**inputs)
logits = outputs.logits

# Convert logits to probabilities using softmax
def compute_probabilities(logits):
    probabilities = torch.softmax(logits, dim=-1)
    return probabilities

probabilities = compute_probabilities(logits)

# Print probabilities for inspection
print(probabilities)

probabilities.shape

probabilities.mean()

probabilities.std()

import matplotlib.pyplot as plt
import numpy as np


# Ensure the data is a PyTorch tensor before applying softmax
if isinstance(probabilities, torch.Tensor):
    probabilities = torch.softmax(probabilities, dim=-1)
else:
    probabilities = torch.tensor(probabilities)
    probabilities = torch.softmax(probabilities, dim=-1)

# Flatten and convert to a NumPy array
probabilities_flat = probabilities.flatten().detach().cpu().numpy()

# Compute the new mean
new_mean = probabilities_flat.mean()
print(f"New Mean: {new_mean}")

# Plot the improved distribution
plt.figure(figsize=(10, 6))
plt.hist(probabilities_flat, bins=50, alpha=0.7, color='green')
plt.xlabel('Probability Value', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Improved Distribution of Probabilities', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-13b-hf")  # Replace with your LLaMA model path

# Load the model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-13b-hf")

# Tokenize input text
input_text = "What is the capital of France"
inputs = tokenizer(input_text, return_tensors="pt")

# Generate model outputs
outputs = model(**inputs)
logits = outputs.logits

# Convert logits to probabilities using softmax
def compute_probabilities(logits):
    probabilities = torch.softmax(logits, dim=-1)
    return probabilities

probabilities_2 = compute_probabilities(logits)

# Print probabilities for inspection
print(probabilities_2)

probabilities_2.shape

probabilities_2.mean()

probabilities_2.std()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the tokenizer
tokenizer_3 = AutoTokenizer.from_pretrained("meta-llama/Llama-2-13b-hf")  # Replace with your LLaMA model path

# Load the model
model3 = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B")

# Tokenize input text
input_text = "What is the capital of France"
inputs = tokenizer_3(input_text, return_tensors="pt")

# Generate model outputs
outputs = mode3(**inputs)
logits = outputs.logits

# Convert logits to probabilities using softmax
def compute_probabilities(logits):
    probabilities = torch.softmax(logits, dim=-1)
    return probabilities

probabilities_3 = compute_probabilities(logits)

# Print probabilities for inspection
print(probabilities_3)

tokenizer

probabilities_3.shape

from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B-Instruct")

# Validate the vocabulary size
assert tokenizer.vocab_size == 32000, f"Expected vocab size 32000, got {tokenizer.vocab_size}"

import json
from transformers import AutoTokenizer

# Load the JSON file
with open('models (1).json', 'r') as file:
    models_data = json.load(file)

# Iterate over the models
for model_name, model_info in models_data.items():
    try:
        # Check if the path exists and load tokenizer
        path = model_info.get("path")
        if isinstance(path, dict):  # Handle paths with 'base' and 'peft'
            path = path.get("base")

        if path:
            tokenizer = AutoTokenizer.from_pretrained(path)
            vocab_size = tokenizer.vocab_size
            models_data[model_name]['vocab_size'] = vocab_size
            print(f"{model_name}: Vocab size is {vocab_size}")
        else:
            print(f"{model_name}: Path not provided. Skipping...")
            models_data[model_name]['vocab_size'] = None
    except Exception as e:
        print(f"{model_name}: Error - {str(e)}")
        models_data[model_name]['vocab_size'] = None

# Save the updated JSON
with open('updated_models.json', 'w') as file:
    json.dump(models_data, file, indent=4)

print("Updated JSON file saved as 'updated_models.json'.")